---
name: integrateLLMChatApp
description: Integrate an on-device MLX LLM into an iOS SwiftUI app with chat and history tabs.
argument-hint: Provide the LLM reference implementation path and target app structure
---
# Integrate On-Device LLM Chat into iOS App

You are helping integrate an MLX-based LLM reference implementation into a SwiftUI app with SwiftData persistence. The goal is to create a chat app where users can interact with an on-device AI and view their chat history.

## Architecture

### Two-Tab Interface
- **Chat Tab** (default): Active chat session with message input, streaming AI responses, and model status
- **History Tab**: Lists all chat sessions grouped by date with swipe-to-delete and session detail view

### SwiftData Models
- **ChatSession**: id, createdAt, lastMessageAt, title, messages relationship, computed `isFrozen` property (read-only for past days)
- **ChatMessage**: id, content, isUser (boolean), timestamp, session relationship
- Use `@Relationship(deleteRule: .cascade)` for automatic message cleanup

### LLM Service Pattern
- Wrap LLM functionality in an `@Observable` service class
- Use `LoadState` enum: `.idle`, `.loading`, `.loaded(LLMModelContainer)`, `.failed(Error)`
- Handle type conflicts with SwiftData by using type aliases:
  ```swift
  typealias LLMModelContext = MLXLMCommon.ModelContext
  typealias LLMModelContainer = MLXLMCommon.ModelContainer
  typealias DataModelContext = SwiftData.ModelContext
  ```

### Key Implementation Details
1. **GPU Memory**: Set cache limit with `MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)`
2. **Streaming**: Use `stream._throttle(for: .seconds(0.25), reducing: Generation.collect)`
3. **UI Updates**: Use `Task { @MainActor in }` for cross-actor state updates
4. **Cancellation**: Support generation cancellation with `Task` and `checkCancellation()`

## Required Package Dependencies
- `mlx-swift` (0.29.x) - MLX framework
- `mlx-swift-lm` (2.29.x) - MLXLLM, MLXLMCommon libraries
- `swift-async-algorithms` - For `_throttle` on async streams  
- `swift-markdown-ui` - For rendering markdown in chat bubbles

## Required Entitlements
```xml
<key>com.apple.developer.kernel.increased-memory-limit</key>
<true/>
<key>com.apple.security.network.client</key>
<true/>
```

## Minimum Deployment Target
- iOS 17+ (required for SwiftData and @Observable)

## File Structure
```
App/
├── Models/
│   ├── ChatSession.swift
│   └── ChatMessage.swift
├── Services/
│   └── LLMService.swift
├── ViewModels/
│   └── DeviceStat.swift (GPU monitoring)
├── Views/
│   ├── MainTabView.swift
│   ├── ChatView.swift
│   └── HistoryView.swift
├── AppName.entitlements
└── AppNameApp.swift
```

## App Entry Point Pattern
```swift
@main
struct AppNameApp: App {
    var sharedModelContainer: ModelContainer = {
        let schema = Schema([ChatSession.self, ChatMessage.self])
        let config = ModelConfiguration(schema: schema, isStoredInMemoryOnly: false)
        return try! ModelContainer(for: schema, configurations: [config])
    }()

    var body: some Scene {
        WindowGroup {
            MainTabView()
                .environment(DeviceStat())
        }
        .modelContainer(sharedModelContainer)
    }
}
```

## Notes
- First launch downloads model (~2GB) - show progress in UI
- Use Release builds for large models to avoid Debug stack overflow
- Consider disabling debugger for performance testing
